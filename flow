
### running code:


### load hyperparameters
task = 'ALFworld'
lr, epoch, batch_size = 1e-3, 10, 64
train_num, test_num = 10, 100
gamma, n_step, target_freq = 0.9, 3, 320
buffer_size = 20000
eps_train, eps_test = 0.1, 0.05
step_per_epoch, step_per_collect = 10000, 10

### load initialize logger
logger = utils.TensorboardLogger(SummaryWriter('log/dqn'))

### load environment
train_envs = ts.env.DummyVectorEnv([lambda: gym.make(task) for _ in range(train_num)])
test_envs = ts.env.DummyVectorEnv([lambda: gym.make(task) for _ in range(test_num)])

### setup policy and collectors
policy = policy.PPOPolicy(
    model=net,
    optim=optim,
    discount_factor=gamma, 
    action_space=env.action_space,
    estimation_step=n_step,
    target_update_freq=target_freq
)
train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), exploration_noise=True)
test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method

### Training (go to trainer.py line.469 )
result = trainer.OnpolicyTrainer(
    policy=policy,
    train_collector=train_collector,
    test_collector=test_collector,
    max_epoch=epoch,
    step_per_epoch=step_per_epoch,
    step_per_collect=step_per_collect,
    episode_per_test=test_num,
    batch_size=batch_size,
    update_per_step=1 / step_per_collect,
    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),
    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),
    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,
    logger=logger,
).run()

### go trainer.py line.38)] (load BaseTrainer)
### go trainer.py line.161 (initialize or reset a new iterator from zero)
### go collector.py Line 39 CollectStats, and then go Line.56 Collector (collect data and save it in buffer)
### go base.py Line 34 TrainingStats (sample minibatch and update)
### go trainer.py Line.462 (perform one on-policy update)
### go trainer.py Line 299 (after every n training steps, perform one test step)
#   Note: the test step can tell us whether the policy is 
#           getting better or worse, and we can define the 
#           "Ground Truth" on the test result)
### go plot_polocy.py Line.191(visualize the policy update)

### if the training is done, save the model and close the logger